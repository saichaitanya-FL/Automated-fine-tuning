{
  "dataset": {
    "type": "file",
    "name": "finance_bench_test_5",
    "path": "train_data_5k.csv",
    "hf_token": "",
    "splitter": "csv",
    "input_fields": ["question"],
    "output_fields": ["answer"],
    "batch_config":{
      "first_batch": 0.10, 
      "second_batch": 0.20,
      "third_batch": 0.30,
      "test_batch": 0.05
    },
    "pdf_config":{
      "llm_config": {
      "api_base":"https://dev-gateway.flotorch.cloud/api/openai/v1",
      "api_key":"sk_5/1GscgXqRJXpthDj4z6d6xxygag9OSg21X7oV93ujU=_NzU2YzZlMmQtMzNkMi00N2Y1LWJiNjctM2M5NDgwODM4NzVk_MTQ0NDdmZGMtNWNiZC00ZGQ1LThjM2EtOTZmNDEwMzI5YzA0",
      "model_name":"flotorch/gpt-4o-mini"
    },
    "chunk_size":2048,
      "overlap":200,
      "qa_pairs_per_chunk": 3,
      "max_generation_tokens": 512
    }
  },

  "output_dir": "results",

  "system_prompt": "You are a highly specialized financial analysis assistant. Your sole purpose is to provide direct, factual, and accurate answers to financial questions. You have been trained on a set of financial Q&A pairs and your task is to replicate the knowledge and format from that training.\nStrict Rules:\nFactual and Precise: Your responses must be factual, precise, and professional. Use appropriate financial terminology (e.g., revenue, operating income, cash flow, liabilities) and format numerical data clearly (e.g., \"$12.3 million\", \"15%\").\nNo Outside Knowledge: You are strictly forbidden from adding any information not present in your training data.\nNo Speculation: Do not infer, speculate, or add any personal opinions.\nHandling Unanswerable Questions: If you do not have a trained answer for the question, you must respond with a single, unembellished sentence: \"I don't have enough information to answer this question.\"\nExample Input Format for Training:\nUser:\nWhat was the net income for Alphabet in Q4 2023?\nAssistant:\nAlphabet's net income for Q4 2023 was $20.7 billion.",

  "experiments": {
    "exp1": {
      "run_always": true,
      "train_batch": "first_batch",
      "model": {
        "model_name": "unsloth/Qwen3-0.6B",
        "rank": 32,
        "alpha": 64,
        "max_seq_len": 2048,
        "exp": 1,
        "dropout": 0,
        "chat_template": "qwen3-instruct"
      },
      "sft": {
        "learning_rate": 2e-5,
        "batch_size": 8,
        "epochs": 2,
        "early_stopping_criteria": false,
        "logging_steps": 50,
        "eval_accumulation_steps": 30,
        "save_steps": 50,
        "eval_steps": 50
      },
      "rules": []
    },

    "exp2": {
      "run_always": true,
      "train_batch": "second_batch",
      "model": {
        "model_name": "unsloth/Qwen3-0.6B",
        "rank": 32,
        "alpha": 64,
        "max_seq_len": 2048,
        "exp": 2,
        "dropout": 0,
        "chat_template": "qwen3-instruct"
      },
      "sft": {
        "learning_rate": 2e-5,
        "batch_size": 8,
        "epochs": 2,
        "early_stopping_criteria": true,
        "logging_steps": 50,
        "eval_accumulation_steps": 30,
        "save_steps": 50,
        "eval_steps": 50
      },
      "rules": []
    },
    "exp2.1": {
      "run_always": true,
      "train_batch": "second_batch",
      "model": {
        "model_name": "unsloth/Qwen3-0.6B",
        "rank": 32,
        "alpha": 64,
        "max_seq_len": 2048,
        "exp": 2,
        "dropout": 0,
        "chat_template": "qwen3-instruct"
      },
      "sft": {
        "learning_rate": 2e-6,
        "batch_size": 8,
        "epochs": 2,
        "early_stopping_criteria": true,
        "logging_steps": 50,
        "eval_accumulation_steps": 30,
        "save_steps": 50,
        "eval_steps": 50
      },
      "rules": [
        {
          "conditions": [
            { "left": "exp2.last_train_loss", "op": ">", "right": "exp2.min_train_loss" }
          ]
        }
      ]
    },

    "exp3.1": {
      "run_always": false,
      "train_batch": "second_batch",
      "model": {
        "model_name": "unsloth/Qwen3-0.6B",
        "rank": 32,
        "alpha": 64,
        "max_seq_len": 2048,
        "exp": 3.1,
        "dropout": 0,
        "chat_template": "qwen3-instruct"
      },
      "sft": {
        "learning_rate": 2e-5,
        "batch_size": 8,
        "epochs": 2,
        "early_stopping_criteria": false,
        "logging_steps": 50,
        "eval_accumulation_steps": 30,
        "save_steps": 50,
        "eval_steps": 50
      },
      "rules": [
        {
          "conditions": [
            { "left": "exp1.f1",              "op": ">",  "right": "exp2.f1" },
            { "left": "exp2.last_eval_loss",  "op": ">",  "right": "exp2.min_eval_loss" },
            { "left": "exp2.last_train_loss", "op": "<=", "right": "exp2.min_train_loss" }
          ]
        }
      ]
    },

    "exp3.2": {
      "run_always": false,
      "train_batch": "third_batch",
      "model": {
        "model_name": "unsloth/Qwen3-0.6B",
        "rank": 32,
        "alpha": 64,
        "max_seq_len": 2048,
        "exp": 3.2,
        "dropout": 0,
        "chat_template": "qwen3-instruct"
      },
      "sft": {
        "learning_rate": 5e-5,
        "batch_size": 8,
        "epochs": 4,
        "early_stopping_criteria": true,
        "logging_steps": 50,
        "eval_accumulation_steps": 30,
        "save_steps": 50,
        "eval_steps": 50
      },
      "rules": [
        {
          "conditions": [
            { "left": "exp1.f1",              "op": "<",  "right": "exp2.f1" },
            { "left": "exp2.last_train_loss", "op": "<=", "right": "exp2.min_train_loss" },
            { "left": "exp2.last_eval_loss", "op": "<=", "right": "exp2.min_eval_loss" }
          ]
        }
      ]
    }
  }
}