# Automated Fine-tuning System

A modular, production-ready system for automated LLM fine-tuning with rule-based experiment execution and intelligent model selection. Cross-platform compatible (Windows, Linux, macOS) with automatic GPU detection and CUDA support.

## üåü Features

- **üöÄ Automated Fine-tuning**: Seamless integration with Unsloth for efficient model training
- **üìä Rule-based Execution**: Smart experiment orchestration based on custom conditions
- **üéØ Multi-model Support**: Compatible with Gemma, Qwen, and other popular models
- **üìà Comprehensive Metrics**: Precision, Recall, F1 Score, and Latency tracking
- **üîÑ PDF-to-QA Generation**: Automatic dataset creation from PDF documents
- **üíæ Dataset-specific Results**: Organized output with date tracking per dataset
- **üåê Cross-platform**: Works on Windows, Linux, macOS, Google Colab, and Cloud environments
- **üìù Advanced Logging**: Detailed logging system for debugging and monitoring
- **üñ•Ô∏è GPU Optimization**: Intelligent GPU detection with automatic CUDA-compatible PyTorch installation
- **üíæ Memory Management**: Automatic batch size adjustment for low-memory GPUs

## üìã Table of Contents

- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Project Structure](#project-structure)
- [Configuration](#configuration)
- [Usage](#usage)
- [Output Structure](#output-structure)
- [Platform-Specific Notes](#platform-specific-notes)
- [Advanced Features](#advanced-features)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)
- [License](#license)

## Prerequisites

- **Python 3.8 or higher**
- **NVIDIA GPU** (recommended, 4GB+ VRAM) - Works with any NVIDIA GPU (RTX, GTX, Quadro, etc.)
- **NVIDIA GPU Driver** (required for GPU acceleration)
  - Verify driver installation: `nvidia-smi` in terminal
  - Download from [NVIDIA Driver Downloads](https://www.nvidia.com/Download/index.aspx)
- **16GB+ RAM** (recommended)
- **Internet connection** (for downloading models and packages)

## üîß Installation

### Step 1: Clone the Repository

```bash
git clone https://github.com/FloTorch/Automated-fine-tuning.git
cd Automated-fine-tuning
```

### Step 2: Create Virtual Environment

**On Windows:**
```powershell
# Create virtual environment
python -m venv venv

# Activate virtual environment
.\venv\Scripts\activate
```

**On Linux/macOS:**
```bash
# Create virtual environment
python3 -m venv venv

# Activate virtual environment
source venv/bin/activate
```

**Note:** After activation, your terminal prompt should show `(venv)` at the beginning.

### Step 3: Install Packages

The system includes an automated package installer that detects your OS and GPU, then installs the appropriate packages:

```bash
# Install all required packages (automatic GPU detection)
python run.py --install-packages
```

**What this command does:**
- ‚úÖ Upgrades pip to the latest version
- ‚úÖ Detects your operating system (Windows/Linux/macOS)
- ‚úÖ Installs Unsloth (with Windows-specific extras if on Windows)
- ‚úÖ Installs `triton-windows==3.3.1.post21` on Windows
- ‚úÖ Detects CUDA version using `nvidia-smi`
- ‚úÖ Installs PyTorch 2.7.1 with appropriate CUDA support
- ‚úÖ Installs torchvision, xformers, and other dependencies
- ‚úÖ Installs additional packages (synthetic-data-kit, openai, sentence-transformers)

**Installation Time:** Approximately 5-15 minutes depending on your internet connection.

**Expected Output:**
```
============================================================
Package Installation with GPU Detection
============================================================
Operating System detected: Windows
Upgrading pip...
Windows detected. Installing unsloth[windows]...
Installing triton-windows==3.3.1.post21 for Windows...
Detecting CUDA version...
CUDA version detected: 11.8
Installing PyTorch 2.7.1 with CUDA 11.8 support
...
Installation completed successfully
============================================================
```

### Step 4: Verify Installation

```bash
# Check if PyTorch is installed correctly
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA Available: {torch.cuda.is_available()}')"

# If GPU is available, you should see:
# PyTorch: 2.7.1+cu118 (or similar)
# CUDA Available: True
```

## üöÄ Quick Start

### Basic Usage

After installation, you can run experiments:

```bash
# Make sure virtual environment is activated
# Windows: .\venv\Scripts\activate
# Linux/macOS: source venv/bin/activate

# Run a single experiment configuration
python run.py configs/config_gemma3.json

# Run multiple configurations
python run.py configs/config_gemma3.json configs/config_qwen3.json
```

### With Custom Parameters

```bash
python run.py configs/config_gemma3.json \
  --threshold-f1 0.3 \
  --threshold-latency 0.5 \
  --factors accuracy latency
```

## üìÅ Project Structure

```
finetuning-system/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ data/                      # Data preparation modules
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data_preparation.py    # Dataset loading and QA generation
‚îÇ   ‚îú‚îÄ‚îÄ models/                    # Model fine-tuning
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ finetuner.py          # Training and evaluation logic
‚îÇ   ‚îú‚îÄ‚îÄ engine/                    # Experiment execution
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rule_engine.py        # Rule-based experiment orchestration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ recommendation_engine.py  # Model selection logic
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                   # Utility functions (package installation, GPU detection)
‚îú‚îÄ‚îÄ configs/                       # Configuration files
‚îÇ   ‚îú‚îÄ‚îÄ config_gemma3.json        # Gemma model configuration
‚îÇ   ‚îú‚îÄ‚îÄ config_qwen3.json         # Qwen model configuration
‚îÇ   ‚îî‚îÄ‚îÄ config_example_with_comments.jsonc
‚îú‚îÄ‚îÄ run.py                         # Main entry point
‚îú‚îÄ‚îÄ requirements.txt               # Python dependencies (reference only)
‚îú‚îÄ‚îÄ finetuning_system.log         # Auto-generated log file
‚îú‚îÄ‚îÄ output/                        # Training checkpoints and outputs
‚îî‚îÄ‚îÄ README.md                      # This file
```

## ‚öôÔ∏è Configuration

### Configuration File Structure

Create a JSON configuration file in the `configs/` directory:

```json
{
  "dataset": {
    "type": "huggingface",
    "name": "databricks/databricks-dolly-15k",
    "path": "databricks/databricks-dolly-15k",
    "hf_token": "your_huggingface_token",
    "splitter": "csv",
    "input_fields": ["instruction", "context"],
    "output_fields": ["response"],
    "batch_config": {
      "first_batch": 0.04,
      "second_batch": 0.06,
      "third_batch": 0.08,
      "test_batch": 0.01
    }
  },
  "output_dir": "results",
  "system_prompt": "You are a helpful assistant.",
  "experiments": {
    "exp1": {
      "run_always": true,
      "train_batch": "first_batch",
      "model": {
        "model_name": "unsloth/gemma-3-270m-it",
        "chat_template": "gemma3",
        "max_seq_len": 2048,
        "rank": 64,
        "alpha": 128,
        "dropout": 0
      },
      "sft": {
        "batch_size": 8,
        "epochs": 2,
        "learning_rate": 2e-5,
        "logging_steps": 50,
        "eval_steps": 50,
        "save_steps": 50,
        "eval_accumulation_steps": 30,
        "early_stopping_criteria": false
      },
      "rules": []
    }
  }
}
```

### Dataset Configuration Options

#### 1. CSV/JSON File
```json
"dataset": {
  "type": "file",
  "path": "data.csv",
  "splitter": "csv",
  "input_fields": ["question"],
  "output_fields": ["answer"]
}
```

#### 2. HuggingFace Dataset
```json
"dataset": {
  "type": "huggingface",
  "name": "databricks/databricks-dolly-15k",
  "path": "databricks/databricks-dolly-15k",
  "hf_token": "your_token_here",
  "input_fields": ["instruction", "context"],
  "output_fields": ["response"]
}
```

#### 3. PDF Documents
```json
"dataset": {
  "type": "file",
  "splitter": "pdf",
  "path": "document.pdf",
  "pdf_config": {
    "llm_config": {
      "api_base": "https://api.openai.com/v1",
      "api_key": "your_api_key",
      "model_name": "gpt-4o-mini"
    },
    "chunk_size": 2048,
    "overlap": 200,
    "qa_pairs_per_chunk": 3,
    "max_generation_tokens": 512
  }
}
```

### Rule-based Experiment Execution

Define conditional experiments based on previous results:

```json
"exp2": {
  "run_always": false,
  "train_batch": "second_batch",
  "rules": [
    {
      "conditions": [
        { "left": "exp1.f1", "op": ">", "right": "exp2.f1" },
        { "left": "exp1.last_eval_loss", "op": "<", "right": "exp1.min_eval_loss" }
      ]
    }
  ]
}
```

**Available Metrics for Rules:**
- `f1`, `precision`, `recall`, `latency`
- `last_eval_loss`, `min_eval_loss`
- `last_train_loss`, `min_train_loss`

**Available Operators:**
- `>`, `<`, `>=`, `<=`, `==`, `!=`

## üíª Usage

### Command-Line Arguments

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `config_files` | str (required) | - | Path(s) to configuration JSON file(s) |
| `--install-packages` | flag | - | Install all required packages with GPU detection |
| `--threshold-f1` | float | 0.2 | F1 score threshold for model selection |
| `--threshold-latency` | float | 0.3 | Latency threshold for model selection |
| `--factors` | list | `['accuracy', 'latency']` | Optimization factors |

### Examples

```bash
# Install packages (first time setup)
python run.py --install-packages

# Basic run
python run.py configs/config_gemma3.json

# Multiple configs with custom thresholds
python run.py configs/config_gemma3.json configs/config_qwen3.json \
  --threshold-f1 0.35 \
  --threshold-latency 0.4

# Optimize for accuracy only
python run.py configs/config_gemma3.json --factors accuracy

# Optimize for latency only
python run.py configs/config_gemma3.json --factors latency
```

## üìä Output Structure

Results are organized by dataset with date tracking:

```
results/
‚îú‚îÄ‚îÄ metrics_dataset_name.csv       # Metrics with date column
‚îú‚îÄ‚îÄ logs_dataset_name_exp1.csv     # Training logs for exp1
‚îú‚îÄ‚îÄ logs_dataset_name_exp2.csv     # Training logs for exp2
‚îî‚îÄ‚îÄ finetuning_system.log          # System logs

output/
‚îî‚îÄ‚îÄ checkpoint-{step}/            # Training checkpoints
    ‚îú‚îÄ‚îÄ adapter_config.json
    ‚îú‚îÄ‚îÄ adapter_model.safetensors
    ‚îî‚îÄ‚îÄ tokenizer files
```

### Metrics CSV Format

| Column | Description |
|--------|-------------|
| `precision` | Token-level precision score |
| `recall` | Token-level recall score |
| `f1` | F1 score (harmonic mean of precision and recall) |
| `latency` | Average inference time in seconds |
| `exp` | Experiment name |
| `model` | Model name |
| `dataset` | Dataset name |
| `date` | Experiment date (YYYY-MM-DD) |

## üåê Platform-Specific Notes

### Windows

**Features:**
- ‚úÖ Automatic installation of `triton-windows==3.3.1.post21`
- ‚úÖ Automatic multiprocessing fixes for Windows compatibility
- ‚úÖ Works with any NVIDIA GPU (RTX, GTX, Quadro, etc.)
- ‚úÖ Automatic memory optimization for low-memory GPUs

**Setup:**
```powershell
# Create and activate venv
python -m venv venv
.\venv\Scripts\activate

# Install packages
python run.py --install-packages

# Run experiments
python run.py configs/config_gemma3.json
```

**Note:** The system automatically:
- Disables optimized loss functions that require Triton compilation
- Forces single-process mode for dataset processing
- Adjusts batch size for GPUs with < 6GB memory

### Linux/macOS

**Setup:**
```bash
# Create and activate venv
python3 -m venv venv
source venv/bin/activate

# Install packages
python run.py --install-packages

# Run experiments
python run.py configs/config_gemma3.json
```

### Google Colab

```python
# Clone repository
!git clone <your-repo-url>
%cd finetuning-system

# Install packages
!python run.py --install-packages

# Run experiments
!python run.py configs/config_gemma3.json
```

### Kaggle Notebooks

1. **Enable GPU**: Settings ‚Üí Accelerator ‚Üí GPU T4 x2
2. **Enable Internet**: Settings ‚Üí Internet ‚Üí ON
3. **Upload files** to `/kaggle/working/`
4. **Run**:

```python
!python /kaggle/working/finetuning-system/run.py --install-packages
!python /kaggle/working/finetuning-system/run.py configs/config_gemma3.json
```

## üî¨ Advanced Features

### GPU Support

The system automatically:
- ‚úÖ Detects CUDA-enabled GPUs (any NVIDIA GPU)
- ‚úÖ Determines compatible CUDA version from driver
- ‚úÖ Installs PyTorch with appropriate CUDA support
- ‚úÖ Falls back to CPU if no GPU available
- ‚úÖ Logs GPU memory and CUDA version
- ‚úÖ Uses mixed precision training (FP16/BF16)
- ‚úÖ Automatically adjusts batch size for low-memory GPUs (< 6GB)

### Memory Management

For GPUs with less than 6GB memory:
- Automatically reduces batch size
- Increases gradient accumulation to maintain effective batch size
- Enables memory-efficient allocation

### Logging System

All operations are logged to:
- **Console**: Real-time output
- **File**: `finetuning_system.log` (persistent)

Log format: `YYYY-MM-DD HH:MM:SS - LEVEL - MESSAGE`

### Evaluation Metrics

1. **Token F1 Score**: Measures token-level overlap between prediction and reference
2. **Semantic Accuracy**: Uses sentence embeddings for semantic similarity
3. **Latency**: Tracks inference time per sample
4. **Training Metrics**: Loss curves and evaluation metrics

### Early Stopping

Enable early stopping to prevent overfitting:

```json
"sft": {
  "early_stopping_criteria": true
}
```

Stops training if evaluation loss doesn't improve for 5 consecutive evaluations.

## üêõ Troubleshooting

### Common Issues

#### 1. CUDA Out of Memory

**Solution:**
- The system automatically reduces batch size for GPUs < 6GB
- Manually reduce batch size in config: `"batch_size": 2` or `1`
- Reduce `max_seq_len` in model config

#### 2. Package Installation Fails

**Solution:**
```bash
# Upgrade pip first
python -m pip install --upgrade pip

# Try installing packages again
python run.py --install-packages

# If specific package fails, install manually:
pip install <package-name>
```

#### 3. Import Errors

**Solution:**
```bash
# Make sure virtual environment is activated
# Windows: .\venv\Scripts\activate
# Linux/macOS: source venv/bin/activate

# Reinstall packages
python run.py --install-packages
```

#### 4. GPU Not Detected

**Solution:**
```bash
# Verify NVIDIA driver is installed
nvidia-smi

# If nvidia-smi fails, install NVIDIA drivers:
# Windows: https://www.nvidia.com/Download/index.aspx
# Linux: Use your distribution's package manager
```

#### 5. Windows-Specific Issues

**Triton Compilation Errors:**
- Automatically handled by the system
- The system disables optimized loss functions on Windows
- Uses standard PyTorch cross-entropy loss instead

**Multiprocessing Errors:**
- Automatically handled by the system
- Forces single-process mode on Windows

#### 6. Slow Training

**Solutions:**
- Ensure GPU is being used (check logs for "GPU detected")
- Reduce `max_seq_len` in model config
- Increase `batch_size` if memory allows
- Check GPU utilization: `nvidia-smi` during training

### Debug Mode

Enable detailed logging:

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## ü§ù Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Development Guidelines

- Follow PEP 8 style guide
- Add docstrings to all functions
- Include unit tests for new features
- Update README.md with new features

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üìß Contact

For questions, issues, or suggestions:
- Open an issue on GitHub
- Contact: [your-email@example.com]

## üôè Acknowledgments

- [Unsloth](https://github.com/unslothai/unsloth) for efficient fine-tuning
- [HuggingFace](https://huggingface.co/) for transformers and datasets
- [TRL](https://github.com/huggingface/trl) for SFT training

## üìö Citation

If you use this system in your research, please cite:

```bibtex
@software{automated_finetuning_system,
  title = {Automated Fine-tuning System},
  author = {Your Name},
  year = {2024},
  url = {https://github.com/your-username/finetuning-system}
}
```

---

**Made with ‚ù§Ô∏è for the AI community**
